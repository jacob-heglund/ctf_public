{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OH(x, l):\n",
    "    x = torch.LongTensor([[x]])\n",
    "    one_hot = torch.FloatTensor(1,l)\n",
    "    return one_hot.zero_().scatter_(1,x,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "#env = gym.make('FrozenLake-v0')\n",
    "\n",
    "# Chance of random action\n",
    "e = 0.1\n",
    "learning_rate = 0.01\n",
    "# Discount Rate\n",
    "gamma = 0.99\n",
    "# Training Episodes\n",
    "episodes = 1000\n",
    "# Max Steps per episode\n",
    "steps = 99\n",
    "\n",
    "\n",
    "# Initialize history memory\n",
    "step_list = []\n",
    "reward_list = []\n",
    "loss_list = []\n",
    "e_list = []\n",
    "\n",
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "\n",
    "model = nn.Sequential(nn.Linear(state_space, action_space, bias=False))\n",
    "loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in trange(episodes):\n",
    "    state = int(env.reset())\n",
    "    reward_all = 0\n",
    "    done = False\n",
    "    s = 0\n",
    "    l = 0\n",
    "    \n",
    "    for s in range(steps):\n",
    "        if np.random.rand(1) < e:\n",
    "            state = Variable(OH(state,state_space))\n",
    "            Q = model(state)\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state = Variable(OH(state,state_space))\n",
    "            Q = model(state)\n",
    "            _, action = torch.max(Q,1)\n",
    "            action = action.data[0]\n",
    "        action = int(action)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        if done and reward == 0.0:\n",
    "            reward = -1\n",
    "        \n",
    "        Q1 = model(Variable(OH(new_state, state_space)))\n",
    "        maxQ1, _ = torch.max(Q1.data, 1)\n",
    "        maxQ1 = torch.FloatTensor(maxQ1)\n",
    "        \n",
    "        targetQ = Variable(Q.data, requires_grad=False)\n",
    "        targetQ[0,action] = reward + torch.mul(maxQ1, gamma)\n",
    "        \n",
    "        output = model(state)\n",
    "        train_loss = loss(output, targetQ)\n",
    "        l+=train_loss.data[0]\n",
    "        \n",
    "        model.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        reward_all += reward\n",
    "        state = new_state\n",
    "        \n",
    "        if done == True:\n",
    "            if reward > 0:\n",
    "                e = 1./((i/50)+10)\n",
    "            break\n",
    "    loss_list.append(l/s)\n",
    "    step_list.append(s)\n",
    "    reward_list.append(reward_all)\n",
    "    e_list.append(e)\n",
    "    \n",
    "print('\\nSuccessful episodes: {}'.format(np.sum(np.array(reward_list)>0.0)/episodes))\n",
    "\n",
    "window = int(episodes/10)\n",
    "\n",
    "plt.figure(figsize=[9,16])\n",
    "plt.subplot(411)\n",
    "plt.plot(pd.Series(step_list).rolling(window).mean())\n",
    "plt.title('Step Moving Average ({}-episode window)'.format(window))\n",
    "plt.ylabel('Moves')\n",
    "plt.xlabel('Episode')\n",
    "\n",
    "plt.subplot(412)\n",
    "plt.plot(pd.Series(reward_list).rolling(window).mean())\n",
    "plt.title('Reward Moving Average ({}-episode window)'.format(window))\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Episode')\n",
    "\n",
    "plt.subplot(413)\n",
    "plt.plot(pd.Series(loss_list).rolling(window).mean())\n",
    "plt.title('Loss Moving Average ({}-episode window)'.format(window))\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Episode')\n",
    "\n",
    "plt.subplot(414)\n",
    "plt.plot(e_list)\n",
    "plt.title('Random Action Parameter')\n",
    "plt.ylabel('Chance Random Action')\n",
    "plt.xlabel('Episode')\n",
    "\n",
    "plt.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
