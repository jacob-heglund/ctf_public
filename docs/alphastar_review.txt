Deepmind and Starcraft: A Brief Review of the Alpha* Algorithm
--------------------------------------------------------------
Alphastar was first presented by Deepmind in January 2019.  The promise of the algorithm was that it could defeat Grandmaster-level human players at the real-time strategy (RTS) game, Starcraft II.  This algorithm is of particular interest because it promises the ability to learn long-term strategies against intelligent enemies.  In many ways, the Starcraft environment is similar to the CTF environment we are using, in that they both non-transitive games. In a non-transitive game, different strategies will have strengths and weaknesses and there is no dominant strategy that is guaranteed to work.  A simple example of a non-transitive game is Rock, Paper, Scissors.

The details of Alphastar has not yet been made available to the public, but a paper is being prepared for release in a peer-reviewed journal.  For now,I'm basing much of my information on the Deepmind blog post about the algorithm (https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/).  The specific implementation of Alphastar is also not public, so there are many details that cannot be included in this review.

The algorithm is based on the following components:

Transformer - an attention-based model that, in combination with a convolutional neural network (CNN), may be able to model sequences of data similar to a recurrent neural network

LSTM - Long short term memory, a version of a recurrent neural network that can model sequences of data

Auto-regressive policy head - a learned compression of the state-action space which allows Starcraft to be tractable to RL methods

Centralized Value Baseline - used in multi-agent settings, algorithm uses a centralized critic to estimate action values, decentralized actors for taking actions.  See the COMA (counterfactual multi-agent) policy gradiant algorithm for more information.

Update rule - Actor-critic algorithm called IMPALA (Importance Weighted Actor-Learner Architecture) that uses experience replay, self-imitation learning, and policy distillation.


Since the release of the algorithm, several researchers have pointed out issues with Deepmind's methodology.  For example, in the linked figure (https://storage.googleapis.com/deepmind-live-cms/images/SCII-BlogPost-Fig09.width-1500.png), we have the actions-per-minute (APM) for Alphastar, MaNa (a world-class Starcraft player), and TLO (another world-class Starcraft player).  Ignoring TLO's plot and comparing MaNa and Alphastar, we can clearly see that MaNa's APM has a maximum of around 750, whereas Alphastar's APM peaked at around 1600.  Combined with the fact that Alphastar was able to control it's in-game characters with much higher accuracy than any human, and we can see that Alphastar had an unfair advantage against the human players.  While the results of Alphastar's games playing against world-class players are impressive, Deepmind has not been completely open or honest about the balance of the matches.  Deepmind's primary goal is to build hype and generate funding for their AI research.  They have successfully built hype, but it remains to be seen that the Alphastar algorithm is tested in a more fair manner.  For more information, look at this Reddit post and it's associated Medium post.

https://www.reddit.com/r/MachineLearning/comments/ak3v4i/d_an_analysis_on_how_alphastars_superhuman_speed/
https://blog.usejournal.com/an-analysis-on-how-deepminds-starcraft-2-ai-s-superhuman-speed-could-be-a-band-aid-fix-for-the-1702fb8344d6
https://arstechnica.com/gaming/2019/01/an-ai-crushed-two-human-pros-at-starcraft-but-it-wasnt-a-fair-fight/
